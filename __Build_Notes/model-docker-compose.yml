# ============================================================================
# STACK NAME (e.g. "LOCAL SERVICES REVERSE PROXY")
# STACK DESCRIPTION (e.g. "Reverse proxy for serving local hosted services on internal lan with HTTPS.")
# LIST OF INCLUDED SERVICES (e.g. "reverse-proxy (CADDY)")
# Served via Caddy on the 'localweb' network
# ============================================================================

name: local-services-reverse-proxy # <- Same name as at the top, but lowercase and kebabcase

services:
  # --------------------------------------------------------------------------
  # SERVICE NAME — SERVICE DESCRIPTION (e.g. "CADDY — Reverse proxy with SSL"
  # --------------------------------------------------------------------------
  service-role: # <-  Using the generic service roles as ID rather than container names
    image: some-image
    # container_name: openwebui                                                             # <-  In general, leave blank since using container names prevents scaling containers past 1 instance
    restart: unless-stopped
    expose:
      - "8080" # <-  If needed

    x-meta: # <-  Using extension records to create a consistent metadata set for every service and stack
      name: openwebui # <-  The name of the actual application provideing the service
      category: application # <-  Service category (can be identical to service role)
      description: "Open WebUI with persistent data & model dirs on bind-backed volumes" # <-  Plain english human-readable service description
      documentation: "https://docs.openwebui.com/" # <-  Link to service documentation, if available
      provides: ["chat", "RAG", "speech", "whisper"] # <-  List of provided roles and tools
      tags: ["ai", "chat", "work", "personal"] # <-  Tags for organization and sorting
      dependencies: ["kokoro-web"] # <-  Service dependancies
      internal_url: "https://llama.home" # <-  If an internal url is provided, use that.

    environment:
      # Core runtime
      ENV: prod
      PORT: 8080
      TZ: ${TZ:-America/New_York}

      # Model backends
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      USE_OLLAMA_DOCKER: "false"
      USE_CUDA_DOCKER: "false"
      USE_CUDA_DOCKER_VER: cu128

      # Embeddings / Rerank (defaults ok)
      USE_EMBEDDING_MODEL_DOCKER: sentence-transformers/all-MiniLM-L6-v2
      USE_RERANKING_MODEL_DOCKER: ""

      # Optional OpenAI compat (not used for TTS; Kokoro provides its own endpoint)
      OPENAI_API_BASE_URL: ""
      OPENAI_API_KEY: ""

      # Privacy
      SCARF_NO_ANALYTICS: "true"
      DO_NOT_TRACK: "true"
      ANONYMIZED_TELEMETRY: "false"

      # Whisper + caches pinned into data volume
      WHISPER_MODEL: base
      WHISPER_MODEL_DIR: /app/backend/data/cache/whisper/models
      RAG_EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2
      RAG_RERANKING_MODEL: ""
      SENTENCE_TRANSFORMERS_HOME: /app/backend/data/cache/embedding/models
      HF_HOME: /app/backend/data/cache/embedding/models
      TIKTOKEN_ENCODING_NAME: cl100k_base
      TIKTOKEN_CACHE_DIR: /app/backend/data/cache/tiktoken

      # WebUI auth/session secret (set in .env)
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY:?set WEBUI_SECRET_KEY in .env}

    volumes:
      - openwebui_data:/app/backend/data
      - openwebui_models:/app/backend/models

    depends_on:
      text-to-speech:
        condition: service_started

    healthcheck:
      test:
        ["CMD-SHELL", "wget -qO- http://localhost:8080/ >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

    networks:
      - localweb

    labels:
      - "endash.stack=ai-stack"
      - "endash.service=ai-frontend"
      - "endash.category=application"
      - "endash.backup=critical"
      - "endash.internal_url=https://llama.home"

  # --------------------------------------------------------------------------
  # KOKORO WEB — OpenAI-compatible TTS sidecar
  # --------------------------------------------------------------------------
  text-to-speech:
    image: ghcr.io/eduardolat/kokoro-web:latest
    container_name: kokoro-web
    restart: unless-stopped
    expose:
      - "3000" # reachable by Open WebUI & Caddy on localweb

    x-meta:
      name: kokoro-web
      category: application
      description: "Low-latency, high-quality local TTS (OpenAI-compatible)"
      documentation: "https://github.com/eduardolat/kokoro-web"
      provides: ["text-to-speech", "openai-compatible-tts"]
      internal_url: "http://kokoro-web:3000/api/v1"

    environment:
      # Set in .env — Open WebUI will use this as the Bearer token
      KW_SECRET_API_KEY: ${KOKORO_API_KEY:?set KOKORO_API_KEY in .env}
      TZ: ${TZ:-America/New_York}

    volumes:
      - kokoro_cache:/kokoro/cache

    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

    networks:
      - localweb

    labels:
      - "endash.stack=ai-stack"
      - "endash.service=text-to-speech"
      - "endash.category=application"
      - "endash.backup=optional"

# -----------------------------------------------------------------------------
# NETWORKS
# -----------------------------------------------------------------------------
networks:
  localweb:
    external: true
    name: localweb

# -----------------------------------------------------------------------------
# VOLUMES — bind-backed named volumes under /mnt/data/openweb/…
# -----------------------------------------------------------------------------
volumes:
  openwebui_data:
    driver: local
    labels:
      - "endash.volume=application"
      - "endash.backup=critical"
      - "endash.ssd_safe=true"
    driver_opts:
      type: none
      o: bind
      device: /mnt/data/openweb/openwebui-data

  openwebui_models:
    driver: local
    labels:
      - "endash.volume=models"
      - "endash.backup=important"
      - "endash.ssd_safe=true"
    driver_opts:
      type: none
      o: bind
      device: /mnt/data/openweb/openwebui-models

  kokoro_cache:
    driver: local
    labels:
      - "endash.volume=cache"
      - "endash.backup=optional"
      - "endash.ssd_safe=true"
    driver_opts:
      type: none
      o: bind
      device: /mnt/data/openweb/kokoro-cache

# -----------------------------------------------------------------------------
# EN-DASH STACK METADATA / CADDY HINTS
# -----------------------------------------------------------------------------
x-endash-config: # <-  Same general pattern as service meta fields, but about the stack as a whole
  stack_name: "ai-stack"
  stack_version: "1.0.0-2025"
  reverse_proxy: "caddy"
  network: "localweb"
  ssl_mode: "internal"
  service_endpoints:
    openwebui: "http://openwebui:8080"
    kokoro_tts: "http://kokoro-web:3000"
  caddy_sites:
    - domain: "ai.home"
      upstream: "http://openwebui:8080"
      notes: "Primary Open WebUI UI/API"
    - domain: "tts.home" # optional (normally not exposed)
      upstream: "http://kokoro-web:3000"
      notes: "Kokoro API (OpenAI-compatible); keep internal if possible"
